{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. データセットの読み込みと正規化\n",
    "'''\n",
    "# tensorflow.keras のインポート\n",
    "from tensorflow import keras\n",
    "\n",
    "# Fashion-MNISTデータセットの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# 訓練データを正規化\n",
    "x_train = x_train.astype('float32') / 255\n",
    "# テストデータを正規化\n",
    "x_test =  x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2.モデルの作成\n",
    "'''\n",
    "class RNN(keras.Model):\n",
    "    '''畳み込みニューラルネットワーク\n",
    "    \n",
    "    Attributes:\n",
    "      i(Conv2D): 入力層\n",
    "      l1(Conv2D): LSTM層1\n",
    "      l2(Conv2D): LSTM層2\n",
    "      l3(Conv2D): LSTM層3\n",
    "      o(Dense): 出力層\n",
    "    '''\n",
    "    def __init__(self, output_dim):\n",
    "        '''\n",
    "        Parameters:\n",
    "          output_dim(int): 出力層のユニット数(次元)\n",
    "                  '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # 入力層\n",
    "        self.i = keras.layers.InputLayer(\n",
    "            input_shape=(28,28)            # 入力の形状は(28,28)\n",
    "            )\n",
    "        \n",
    "        #  LSTM（ユニット数＝128）\n",
    "        weight_decay = 1e-4  # ハイパーパラメーター\n",
    "        self.l1 = keras.layers.LSTM(units=128, dropout=0.25, return_sequences=True) \n",
    "        self.l2 = keras.layers.LSTM(units=128, dropout=0.25, return_sequences=True)\n",
    "        self.l3 = keras.layers.LSTM(\n",
    "            units=128, dropout=0.5, return_sequences=False,\n",
    "            kernel_regularizer=keras.regularizers.l2(weight_decay)) # 正則化\n",
    "\n",
    "        # 出力層\n",
    "        self.o =  keras.layers.Dense(units=10, # ニューロン数は10\n",
    "          activation='softmax')                # 活性化はソフトマックス関数\n",
    "        \n",
    "        # すべての層をリストにする\n",
    "        self.ls = [self.i, self.l1, self.l2, self.l3, self.o]\n",
    "\n",
    "    def call(self, x):\n",
    "        '''MLPのインスタンスからコールバックされる関数\n",
    "        \n",
    "        Parameters: x(ndarray(float32)):訓練データ、または検証データ\n",
    "        Returns(float32): RNNの出力として要素数10の1階テンソル        \n",
    "        '''\n",
    "        for layer in self.ls:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3.損失関数の定義\n",
    "'''\n",
    "# 損失関数はスパースラベル対応クロスエントロピー誤差\n",
    "cce = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def loss(t, y):\n",
    "    '''損失関数\n",
    "    Parameters: t(ndarray(float32)):正解ラベル\n",
    "                y(ndarray(float32)):予測値\n",
    "                \n",
    "    Returns: クロスエントロピー誤差\n",
    "    '''\n",
    "    return cce(t, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4.オプティマイザーのオブジェクトの生成\n",
    "  損失と精度を測定するオブジェクトの生成\n",
    "'''\n",
    "# 勾配降下アルゴリズムを使用するオプティマイザーを生成\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# 損失を記録するオブジェクトを生成\n",
    "train_loss = keras.metrics.Mean()\n",
    "# 精度を記録するオブジェクトを生成\n",
    "train_acc = keras.metrics.SparseCategoricalAccuracy()\n",
    "# 検証時の損失を記録するオブジェクトを生成\n",
    "val_loss = keras.metrics.Mean()\n",
    "# 検証時の精度を記録するオブジェクトを生成\n",
    "val_acc = keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5.勾配降下アルゴリズムによるパラメーターの更新処理\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_step(x, t):\n",
    "    '''学習を1回行う\n",
    "    \n",
    "    Parameters: x(ndarray(float32)):訓練データ\n",
    "                t(ndarray(float32)):正解ラベル\n",
    "                \n",
    "    Returns:\n",
    "      ステップごとのクロスエントロピー誤差\n",
    "    '''\n",
    "    # 自動微分による勾配計算を記録するブロック\n",
    "    with tf.GradientTape() as tape:\n",
    "        # モデルに入力して順伝搬の出力値を取得\n",
    "        outputs = model(x)\n",
    "        # 出力値と正解ラベルの誤差\n",
    "        tmp_loss = loss(t, outputs)\n",
    "        \n",
    "    # tapeに記録された操作を使用して誤差の勾配を計算        \n",
    "    grads = tape.gradient(\n",
    "        # 現在のステップの誤差\n",
    "        tmp_loss,\n",
    "        # バイアス、重みのリストを取得\n",
    "        model.trainable_variables)\n",
    "    # 勾配降下法の更新式を適用してバイアス、重みを更新\n",
    "    optimizer.apply_gradients(zip(grads,\n",
    "                                  model.trainable_variables))\n",
    "    \n",
    "    # 損失をMeanオブジェクトに記録\n",
    "    train_loss(tmp_loss)\n",
    "    # 精度をCategoricalAccuracyオブジェクトに記録\n",
    "    train_acc(t, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. 検証データによる評価を行う関数\n",
    "'''\n",
    "def val_step(x, t):\n",
    "    '''検証データをモデルに入力して損失と精度を測定\n",
    "    \n",
    "    Parameters: x(ndarray(float32)):検証データ\n",
    "                t(ndarray(float32)):正解ラベル\n",
    "    '''\n",
    "    # 検証データの予測値を取得\n",
    "    preds = model(x)\n",
    "    # 出力値と正解ラベルの誤差\n",
    "    tmp_loss = loss(t, preds)\n",
    "    # 損失をMeanオブジェクトに記録\n",
    "    val_loss(tmp_loss)\n",
    "    # 精度をCategoricalAccuracyオブジェクトに記録\n",
    "    val_acc(t, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "7. 訓練データの20パーセントのデータを検証用のデータにする\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 訓練データと検証データに8：2の割合で分割  \\は行継続文字\n",
    "x_train, x_val, t_train, t_val = \\\n",
    "    train_test_split(x_train, t_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch(1) train_loss: 0.8413 train_acc: 0.6916val_loss: 0.5617 val_acc: 0.7894\n",
      "epoch(2) train_loss: 0.6929 train_acc: 0.7434val_loss: 0.5227 val_acc: 0.8028\n",
      "epoch(3) train_loss: 0.6202 train_acc: 0.7695val_loss: 0.4914 val_acc: 0.8145\n",
      "epoch(4) train_loss: 0.5735 train_acc: 0.786val_loss: 0.4685 val_acc: 0.8235\n",
      "epoch(5) train_loss: 0.5395 train_acc: 0.7984val_loss: 0.4512 val_acc: 0.8304\n",
      "epoch(6) train_loss: 0.5128 train_acc: 0.8085val_loss: 0.4357 val_acc: 0.837\n",
      "epoch(7) train_loss: 0.491 train_acc: 0.8167val_loss: 0.4227 val_acc: 0.8424\n",
      "epoch(8) train_loss: 0.4727 train_acc: 0.8237val_loss: 0.4115 val_acc: 0.8469\n",
      "epoch(9) train_loss: 0.457 train_acc: 0.8295val_loss: 0.4015 val_acc: 0.8507\n",
      "epoch(10) train_loss: 0.4432 train_acc: 0.8347val_loss: 0.3928 val_acc: 0.854\n",
      "epoch(11) train_loss: 0.4309 train_acc: 0.8393val_loss: 0.3848 val_acc: 0.857\n",
      "epoch(12) train_loss: 0.4199 train_acc: 0.8434val_loss: 0.3772 val_acc: 0.8597\n",
      "epoch(13) train_loss: 0.4097 train_acc: 0.8471val_loss: 0.3705 val_acc: 0.8622\n",
      "epoch(14) train_loss: 0.4006 train_acc: 0.8505val_loss: 0.3642 val_acc: 0.8646\n",
      "epoch(15) train_loss: 0.3921 train_acc: 0.8537val_loss: 0.3583 val_acc: 0.8668\n",
      "epoch(16) train_loss: 0.384 train_acc: 0.8567val_loss: 0.353 val_acc: 0.8687\n",
      "epoch(17) train_loss: 0.3765 train_acc: 0.8595val_loss: 0.3477 val_acc: 0.8707\n",
      "epoch(18) train_loss: 0.3693 train_acc: 0.8621val_loss: 0.3425 val_acc: 0.8726\n",
      "epoch(19) train_loss: 0.3626 train_acc: 0.8646val_loss: 0.3382 val_acc: 0.8742\n",
      "epoch(20) train_loss: 0.3563 train_acc: 0.867val_loss: 0.3337 val_acc: 0.8759\n",
      "epoch(21) train_loss: 0.3501 train_acc: 0.8692val_loss: 0.3294 val_acc: 0.8775\n",
      "epoch(22) train_loss: 0.3444 train_acc: 0.8713val_loss: 0.3256 val_acc: 0.8788\n",
      "epoch(23) train_loss: 0.3389 train_acc: 0.8733val_loss: 0.3221 val_acc: 0.88\n",
      "epoch(24) train_loss: 0.3337 train_acc: 0.8753val_loss: 0.3191 val_acc: 0.8812\n",
      "epoch(25) train_loss: 0.3285 train_acc: 0.8771val_loss: 0.3157 val_acc: 0.8824\n",
      "epoch(26) train_loss: 0.3235 train_acc: 0.879val_loss: 0.3124 val_acc: 0.8836\n",
      "epoch(27) train_loss: 0.3186 train_acc: 0.8807val_loss: 0.3096 val_acc: 0.8847\n",
      "epoch(28) train_loss: 0.314 train_acc: 0.8824val_loss: 0.3069 val_acc: 0.8857\n",
      "epoch(29) train_loss: 0.3095 train_acc: 0.8841val_loss: 0.3044 val_acc: 0.8866\n",
      "epoch(30) train_loss: 0.3051 train_acc: 0.8857val_loss: 0.3019 val_acc: 0.8876\n",
      "epoch(31) train_loss: 0.3009 train_acc: 0.8873val_loss: 0.2994 val_acc: 0.8886\n",
      "epoch(32) train_loss: 0.2968 train_acc: 0.8888val_loss: 0.2975 val_acc: 0.8893\n",
      "epoch(33) train_loss: 0.2928 train_acc: 0.8903val_loss: 0.2951 val_acc: 0.8902\n",
      "epoch(34) train_loss: 0.2888 train_acc: 0.8918val_loss: 0.2927 val_acc: 0.8911\n",
      "epoch(35) train_loss: 0.2848 train_acc: 0.8932val_loss: 0.2903 val_acc: 0.8921\n",
      "epoch(36) train_loss: 0.281 train_acc: 0.8947val_loss: 0.2884 val_acc: 0.8927\n",
      "epoch(37) train_loss: 0.2773 train_acc: 0.8961val_loss: 0.2862 val_acc: 0.8936\n",
      "epoch(38) train_loss: 0.2736 train_acc: 0.8974val_loss: 0.2843 val_acc: 0.8943\n",
      "epoch(39) train_loss: 0.2704 train_acc: 0.8986val_loss: 0.2826 val_acc: 0.8949\n",
      "epoch(40) train_loss: 0.2672 train_acc: 0.8998val_loss: 0.281 val_acc: 0.8956\n",
      "epoch(41) train_loss: 0.2637 train_acc: 0.9012val_loss: 0.2789 val_acc: 0.8964\n",
      "epoch(42) train_loss: 0.2603 train_acc: 0.9025val_loss: 0.2768 val_acc: 0.8971\n",
      "epoch(43) train_loss: 0.2572 train_acc: 0.9036val_loss: 0.2749 val_acc: 0.8978\n",
      "epoch(44) train_loss: 0.254 train_acc: 0.9048val_loss: 0.2728 val_acc: 0.8987\n",
      "epoch(45) train_loss: 0.2509 train_acc: 0.906val_loss: 0.2713 val_acc: 0.8993\n",
      "epoch(46) train_loss: 0.2478 train_acc: 0.9071val_loss: 0.2698 val_acc: 0.8999\n",
      "epoch(47) train_loss: 0.2449 train_acc: 0.9082val_loss: 0.2678 val_acc: 0.9006\n",
      "epoch(48) train_loss: 0.2421 train_acc: 0.9093val_loss: 0.266 val_acc: 0.9013\n",
      "epoch(49) train_loss: 0.2392 train_acc: 0.9104val_loss: 0.2645 val_acc: 0.9019\n",
      "epoch(50) train_loss: 0.2364 train_acc: 0.9115val_loss: 0.2632 val_acc: 0.9025\n",
      "epoch(51) train_loss: 0.2337 train_acc: 0.9125val_loss: 0.2617 val_acc: 0.9031\n",
      "epoch(52) train_loss: 0.231 train_acc: 0.9135val_loss: 0.2602 val_acc: 0.9037\n",
      "epoch(53) train_loss: 0.2284 train_acc: 0.9145val_loss: 0.2582 val_acc: 0.9045\n",
      "epoch(54) train_loss: 0.2258 train_acc: 0.9155val_loss: 0.2563 val_acc: 0.9053\n",
      "epoch(55) train_loss: 0.2231 train_acc: 0.9165val_loss: 0.2544 val_acc: 0.906\n",
      "epoch(56) train_loss: 0.2206 train_acc: 0.9175val_loss: 0.253 val_acc: 0.9066\n",
      "epoch(57) train_loss: 0.2183 train_acc: 0.9184val_loss: 0.2514 val_acc: 0.9072\n",
      "epoch(58) train_loss: 0.2158 train_acc: 0.9193val_loss: 0.2498 val_acc: 0.9079\n",
      "epoch(59) train_loss: 0.2133 train_acc: 0.9203val_loss: 0.248 val_acc: 0.9086\n",
      "epoch(60) train_loss: 0.2109 train_acc: 0.9212val_loss: 0.2467 val_acc: 0.9092\n",
      "epoch(61) train_loss: 0.2087 train_acc: 0.922val_loss: 0.2448 val_acc: 0.9099\n",
      "epoch(62) train_loss: 0.2064 train_acc: 0.9229val_loss: 0.2431 val_acc: 0.9106\n",
      "epoch(63) train_loss: 0.2042 train_acc: 0.9237val_loss: 0.2419 val_acc: 0.9111\n",
      "epoch(64) train_loss: 0.2019 train_acc: 0.9246val_loss: 0.2402 val_acc: 0.9118\n",
      "epoch(65) train_loss: 0.1997 train_acc: 0.9254val_loss: 0.2384 val_acc: 0.9126\n",
      "epoch(66) train_loss: 0.1975 train_acc: 0.9263val_loss: 0.2367 val_acc: 0.9132\n",
      "epoch(67) train_loss: 0.1955 train_acc: 0.927val_loss: 0.2352 val_acc: 0.9138\n",
      "epoch(68) train_loss: 0.1934 train_acc: 0.9278val_loss: 0.2333 val_acc: 0.9146\n",
      "epoch(69) train_loss: 0.1914 train_acc: 0.9286val_loss: 0.2318 val_acc: 0.9152\n",
      "epoch(70) train_loss: 0.1894 train_acc: 0.9294val_loss: 0.2303 val_acc: 0.9158\n",
      "epoch(71) train_loss: 0.1876 train_acc: 0.9301val_loss: 0.2288 val_acc: 0.9164\n",
      "epoch(72) train_loss: 0.1856 train_acc: 0.9308val_loss: 0.2275 val_acc: 0.917\n",
      "epoch(73) train_loss: 0.1839 train_acc: 0.9315val_loss: 0.2264 val_acc: 0.9174\n",
      "epoch(74) train_loss: 0.182 train_acc: 0.9322val_loss: 0.2249 val_acc: 0.918\n",
      "epoch(75) train_loss: 0.1802 train_acc: 0.9328val_loss: 0.2234 val_acc: 0.9186\n",
      "epoch(76) train_loss: 0.1785 train_acc: 0.9335val_loss: 0.2222 val_acc: 0.9191\n",
      "epoch(77) train_loss: 0.1767 train_acc: 0.9342val_loss: 0.2206 val_acc: 0.9197\n",
      "epoch(78) train_loss: 0.175 train_acc: 0.9348val_loss: 0.2192 val_acc: 0.9203\n",
      "epoch(79) train_loss: 0.1734 train_acc: 0.9355val_loss: 0.2178 val_acc: 0.9208\n",
      "epoch(80) train_loss: 0.1717 train_acc: 0.9361val_loss: 0.2167 val_acc: 0.9213\n",
      "epoch(81) train_loss: 0.1701 train_acc: 0.9367val_loss: 0.2154 val_acc: 0.9219\n",
      "epoch(82) train_loss: 0.1685 train_acc: 0.9373val_loss: 0.214 val_acc: 0.9224\n",
      "epoch(83) train_loss: 0.1668 train_acc: 0.938val_loss: 0.2126 val_acc: 0.9229\n",
      "epoch(84) train_loss: 0.1652 train_acc: 0.9386val_loss: 0.2114 val_acc: 0.9234\n",
      "epoch(85) train_loss: 0.1635 train_acc: 0.9392val_loss: 0.2101 val_acc: 0.924\n",
      "epoch(86) train_loss: 0.1619 train_acc: 0.9398val_loss: 0.2087 val_acc: 0.9245\n",
      "epoch(87) train_loss: 0.1604 train_acc: 0.9404val_loss: 0.2079 val_acc: 0.9249\n",
      "epoch(88) train_loss: 0.159 train_acc: 0.941val_loss: 0.2068 val_acc: 0.9254\n",
      "epoch(89) train_loss: 0.1576 train_acc: 0.9415val_loss: 0.2057 val_acc: 0.9258\n",
      "epoch(90) train_loss: 0.1563 train_acc: 0.942val_loss: 0.2046 val_acc: 0.9263\n",
      "epoch(91) train_loss: 0.1551 train_acc: 0.9425val_loss: 0.2035 val_acc: 0.9267\n",
      "epoch(92) train_loss: 0.1537 train_acc: 0.943val_loss: 0.2027 val_acc: 0.9271\n",
      "epoch(93) train_loss: 0.1524 train_acc: 0.9435val_loss: 0.2015 val_acc: 0.9275\n",
      "epoch(94) train_loss: 0.1511 train_acc: 0.944val_loss: 0.2003 val_acc: 0.928\n",
      "epoch(95) train_loss: 0.1498 train_acc: 0.9445val_loss: 0.1991 val_acc: 0.9285\n",
      "epoch(96) train_loss: 0.1485 train_acc: 0.945val_loss: 0.198 val_acc: 0.9289\n",
      "epoch(97) train_loss: 0.1473 train_acc: 0.9454val_loss: 0.197 val_acc: 0.9293\n",
      "epoch(98) train_loss: 0.1461 train_acc: 0.9459val_loss: 0.1958 val_acc: 0.9298\n",
      "epoch(99) train_loss: 0.145 train_acc: 0.9463val_loss: 0.1946 val_acc: 0.9302\n",
      "epoch(100) train_loss: 0.1438 train_acc: 0.9467val_loss: 0.1934 val_acc: 0.9307\n",
      "Wall time: 8h 29min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "8.モデルを生成して学習する\n",
    "'''\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# エポック数\n",
    "epochs = 100\n",
    "# ミニバッチのサイズ\n",
    "batch_size = 64\n",
    "# 訓練データのステップ数\n",
    "steps = x_train.shape[0] // batch_size\n",
    "# 検証データのステップ数\n",
    "steps_val = x_val.shape[0] // batch_size\n",
    "\n",
    "# 出力層10ニューロンのモデルを生成\n",
    "model = RNN(10)\n",
    "\n",
    "# 学習を行う\n",
    "for epoch in range(epochs):\n",
    "    # 訓練データと正解ラベルをシャッフル\n",
    "    x_, t_ = shuffle(x_train, t_train, random_state=1)\n",
    "    \n",
    "    # 1ステップにおけるミニバッチを使用した学習\n",
    "    for step in range(steps):\n",
    "        start = step * batch_size # ミニバッチの先頭インデックス\n",
    "        end = start + batch_size  # ミニバッチの末尾のインデックス\n",
    "        # ミニバッチでバイアス、重みを更新\n",
    "        train_step(x_[start:end], t_[start:end])\n",
    "        \n",
    "    # 検証データによるモデルの評価\n",
    "    for step_val in range(steps_val):\n",
    "        start = step_val * batch_size # ミニバッチの先頭インデックス\n",
    "        end = start + batch_size      # ミニバッチの末尾のインデックス\n",
    "        # 検証データのミニバッチで損失と精度を測定\n",
    "        val_step(x_[start:end], t_[start:end])\n",
    "\n",
    "    # 1エポックごとに結果を出力\n",
    "    print('epoch({}) train_loss: {:.4} train_acc: {:.4}'\n",
    "          'val_loss: {:.4} val_acc: {:.4}'.format(\n",
    "              epoch+1,\n",
    "              train_loss.result(), # 訓練データの損失を出力\n",
    "              train_acc.result(),  # 訓練データの精度を出力\n",
    "              val_loss.result(),   # 検証データの損失を出力\n",
    "              val_acc.result()     # 検証データの精度を出力\n",
    "              ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.6401, test_acc: 0.8814\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "9. テストデータによるモデルの評価\n",
    "'''\n",
    "# 損失を記録するオブジェクトを生成\n",
    "test_loss = keras.metrics.Mean()\n",
    "# 精度を記録するオブジェクトを生成\n",
    "test_acc = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "def test_step(x, t):\n",
    "    '''テストデータをモデルに入力して損失と精度を測定\n",
    "    \n",
    "    Parameters: x(ndarray(float32)):テストデータ\n",
    "                t(ndarray(float32)):正解ラベル\n",
    "    '''\n",
    "    # テストデータの予測値を取得\n",
    "    preds = model(x)\n",
    "    # 出力値と正解ラベルの誤差\n",
    "    tmp_loss = loss(t, preds)\n",
    "    # 損失をMeanオブジェクトに記録\n",
    "    test_loss(tmp_loss)\n",
    "    # 精度をCategoricalAccuracyオブジェクトに記録\n",
    "    test_acc(t, preds)\n",
    "\n",
    "# テストデータで予測して損失と精度を取得\n",
    "test_step(x_test, t_test)\n",
    "\n",
    "print('test_loss: {:.4f}, test_acc: {:.4f}'.format(\n",
    "    test_loss.result(),\n",
    "    test_acc.result()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
