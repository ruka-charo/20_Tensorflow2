{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. データセットの読み込みと正規化\n",
    "'''\n",
    "# tensorflow.keras のインポート\n",
    "from tensorflow import keras\n",
    "\n",
    "# Fashion-MNISTデータセットの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# 訓練データを正規化\n",
    "x_train = x_train.astype('float32') / 255\n",
    "# テストデータを正規化\n",
    "x_test =  x_test.astype('float32') / 255\n",
    "\n",
    "# (60000, 28, 28)の3階テンソルを(60000, 28, 28, 1)の4階テンソルに変換\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "# (10000, 28, 28)の3階テンソルを(10000, 28, 28, 1)の4階テンソルに変換\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2.モデルの作成\n",
    "'''\n",
    "class CNN(keras.Model):\n",
    "    '''畳み込みニューラルネットワーク\n",
    "    \n",
    "    Attributes:\n",
    "      c1(Conv2D): 畳み込み層1\n",
    "      c2(Conv2D): 畳み込み層2\n",
    "      p1(MaxPooling2D): プーリング層1\n",
    "      c3(Conv2D): 畳み込み層3\n",
    "      p2(MaxPooling2D): プーリング層2\n",
    "      d1(Dropout): ドロップアウト1\n",
    "      f1(Flatten): Flatten\n",
    "      l1(Dense): 全結合層\n",
    "      l2(Dense): 出力層\n",
    "    '''\n",
    "    def __init__(self, output_dim):\n",
    "        '''\n",
    "        Parameters:\n",
    "          output_dim(int): 出力層のユニット数(次元)\n",
    "                  '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # 正則化の係数\n",
    "        weight_decay = 1e-4\n",
    "\n",
    "        # （第1層）畳み込み層1\n",
    "        # ニューロン数：64\n",
    "        # 出力：1ニューロンあたり(28, 28, 1)の3階テンソルを64出力するので\n",
    "        #       (28, 28, 64)の3出力となる\n",
    "        self.c1 = keras.layers.Conv2D(\n",
    "            filters=64,                   # フィルター数64\n",
    "            kernel_size=(3, 3),           # 3×3のフィルター\n",
    "            padding='same',               # ゼロパディング\n",
    "            input_shape=x_train[0].shape, # 入力データの形状\n",
    "            kernel_regularizer=keras.regularizers.l2(\n",
    "                weight_decay), # 正則化\n",
    "            activation='relu'             # 活性化関数はReLU\n",
    "            )\n",
    "        \n",
    "        # （第2層）畳み込み層2\n",
    "        # ニューロン数：32\n",
    "        # 出力：1ニューロンあたり(28, 28, 1)の3階テンソルを32出力するので\n",
    "        #       (28, 28, 32)の出力となる\n",
    "        self.c2 = keras.layers.Conv2D(\n",
    "            filters=32,                   # フィルターの数は32\n",
    "            kernel_size=(3, 3),           # 3×3のフィルターを使用\n",
    "            padding='same',               # ゼロパディングを行う\n",
    "            kernel_regularizer=keras.regularizers.l2(\n",
    "                weight_decay),           # 正則化\n",
    "            activation='relu'             # 活性化関数はReLU\n",
    "            )\n",
    "        \n",
    "        # （第3層）プーリング層1\n",
    "        # ニューロン数：32\n",
    "        # 出力：1ニューロンあたり(14, 14, 1)の3階テンソルを32出力するので\n",
    "        #       (14, 14, 32)の出力となる\n",
    "        self.p1 = keras.layers.MaxPooling2D(\n",
    "            pool_size=(2, 2))             # 縮小対象の領域は2×2\n",
    "        \n",
    "        # （第4層）畳み込み層3\n",
    "        # ニューロン数：16\n",
    "        # 出力：1ニューロンあたり(14 14 1)の3階テンソルを16出力するので\n",
    "        #       (14, 14, 16)の出力となる\n",
    "        self.c3 = keras.layers.Conv2D(\n",
    "            filters=16,                   # フィルターの数は16\n",
    "            kernel_size=(3, 3),           # 3×3のフィルターを使用\n",
    "            padding='same',               # ゼロパディングを行う\n",
    "            kernel_regularizer=keras.regularizers.l2(weight_decacy), # 正則化\n",
    "            activation='relu'             # 活性化関数はReLU\n",
    "            )\n",
    "        \n",
    "        # （第5層）プーリング層2\n",
    "        # ニューロン数：16\n",
    "        # 出力：1ニューロンあたり(7, 7, 1)の3階テンソルを16出力するので\n",
    "        #       (7, 7, 16)の出力となる\n",
    "        self.p2 = keras.layers.MaxPooling2D(\n",
    "            pool_size=(2, 2))             # 縮小対象の領域は2×2\n",
    "\n",
    "        # ドロップアウト40％\n",
    "        self.d1 = keras.layers.Dropout(0.4)\n",
    "        \n",
    "        # Flaten\n",
    "        # ニューロン数＝7×7×16=784\n",
    "        # (7, 7, 64)を(784)にフラット化\n",
    "        self.f1 = keras.layers.Flatten()\n",
    "\n",
    "        # （第6層）全結合層\n",
    "        # ニューロン数：128\n",
    "        # 出力：(128)の1階テンソルを出力\n",
    "        self.l1 =  keras.layers.Dense(\n",
    "            128,                          # ニューロン数は128\n",
    "            activation='relu')            # 活性化関数はReLU\n",
    "\n",
    "        # （第7層）出力層\n",
    "        # ニューロン数：10\n",
    "        # 出力：要素数(10)の1階テンソルを出力\n",
    "        self.l2 =  keras.layers.Dense(\n",
    "            output_dim,                   # 出力層のニューロン数は10\n",
    "            activation='softmax')         # 活性化関数はソフトマックス\n",
    "        \n",
    "        # すべての層をリストにする\n",
    "        self.ls = [self.c1, self.c2, self.p1, self.c3,\n",
    "                   self.p2, self.d1, self.f1, self.l1, self.l2]\n",
    "\n",
    "    def call(self, x):\n",
    "        '''MLPのインスタンスからコールバックされる関数\n",
    "        \n",
    "        Parameters: x(ndarray(float32)):訓練データ、または検証データ\n",
    "        Returns(float32): CNNの出力として要素数10の1階テンソル        \n",
    "        '''\n",
    "        for layer in self.ls:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3.損失関数の定義\n",
    "'''\n",
    "# 損失関数はスパースラベル対応クロスエントロピー誤差\n",
    "cce = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def loss(t, y):\n",
    "    '''損失関数\n",
    "    Parameters: t(ndarray(float32)):正解ラベル\n",
    "                y(ndarray(float32)):予測値\n",
    "                \n",
    "    Returns: クロスエントロピー誤差\n",
    "    '''\n",
    "    return cce(t, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4.オプティマイザーのオブジェクトの生成\n",
    "  損失と精度を測定するオブジェクトの生成\n",
    "'''\n",
    "# 勾配降下アルゴリズムを使用するオプティマイザーを生成\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# 損失を記録するオブジェクトを生成\n",
    "train_loss = keras.metrics.Mean()\n",
    "# 精度を記録するオブジェクトを生成\n",
    "train_acc = keras.metrics.SparseCategoricalAccuracy()\n",
    "# 検証時の損失を記録するオブジェクトを生成\n",
    "val_loss = keras.metrics.Mean()\n",
    "# 検証時の精度を記録するオブジェクトを生成\n",
    "val_acc = keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5.勾配降下アルゴリズムによるパラメーターの更新処理\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_step(x, t):\n",
    "    '''学習を1回行う\n",
    "    \n",
    "    Parameters: x(ndarray(float32)):訓練データ\n",
    "                t(ndarray(float32)):正解ラベル\n",
    "                \n",
    "    Returns:\n",
    "      ステップごとのクロスエントロピー誤差\n",
    "    '''\n",
    "    # 自動微分による勾配計算を記録するブロック\n",
    "    with tf.GradientTape() as tape:\n",
    "        # モデルに入力して順伝搬の出力値を取得\n",
    "        outputs = model(x)\n",
    "        # 出力値と正解ラベルの誤差\n",
    "        tmp_loss = loss(t, outputs)\n",
    "        \n",
    "    # tapeに記録された操作を使用して誤差の勾配を計算        \n",
    "    grads = tape.gradient(\n",
    "        # 現在のステップの誤差\n",
    "        tmp_loss,\n",
    "        # バイアス、重みのリストを取得\n",
    "        model.trainable_variables)\n",
    "    # 勾配降下法の更新式を適用してバイアス、重みを更新\n",
    "    optimizer.apply_gradients(zip(grads,\n",
    "                                  model.trainable_variables))\n",
    "    \n",
    "    # 損失をMeanオブジェクトに記録\n",
    "    train_loss(tmp_loss)\n",
    "    # 精度をCategoricalAccuracyオブジェクトに記録\n",
    "    train_acc(t, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. 検証データによる評価を行う関数\n",
    "'''\n",
    "def val_step(x, t):\n",
    "    '''検証データをモデルに入力して損失と精度を測定\n",
    "    \n",
    "    Parameters: x(ndarray(float32)):検証データ\n",
    "                t(ndarray(float32)):正解ラベル\n",
    "    '''\n",
    "    # 検証データの予測値を取得\n",
    "    preds = model(x)\n",
    "    # 出力値と正解ラベルの誤差\n",
    "    tmp_loss = loss(t, preds)\n",
    "    # 損失をMeanオブジェクトに記録\n",
    "    val_loss(tmp_loss)\n",
    "    # 精度をSpase_CategoricalAccuracyオブジェクトに記録\n",
    "    val_acc(t, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "7. 訓練データの20パーセントのデータを検証用のデータにする\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 訓練データと検証データに8：2の割合で分割  \\は行継続文字\n",
    "x_train, x_val, t_train, t_val = \\\n",
    "    train_test_split(x_train, t_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch(1) train_loss: 0.6147 train_acc: 0.7725val_loss: 0.5319 val_acc: 0.7932\n",
      "epoch(2) train_loss: 0.4904 train_acc: 0.8194val_loss: 0.4601 val_acc: 0.8226\n",
      "epoch(3) train_loss: 0.4296 train_acc: 0.8421val_loss: 0.4066 val_acc: 0.8453\n",
      "epoch(4) train_loss: 0.3903 train_acc: 0.8567val_loss: 0.3678 val_acc: 0.8612\n",
      "epoch(5) train_loss: 0.3618 train_acc: 0.8671val_loss: 0.3402 val_acc: 0.872\n",
      "epoch(6) train_loss: 0.3395 train_acc: 0.8752val_loss: 0.3188 val_acc: 0.8802\n",
      "epoch(7) train_loss: 0.3212 train_acc: 0.882val_loss: 0.301 val_acc: 0.887\n",
      "epoch(8) train_loss: 0.3057 train_acc: 0.8877val_loss: 0.2862 val_acc: 0.8926\n",
      "epoch(9) train_loss: 0.2921 train_acc: 0.8928val_loss: 0.2737 val_acc: 0.8972\n",
      "epoch(10) train_loss: 0.2798 train_acc: 0.8973val_loss: 0.263 val_acc: 0.9011\n",
      "epoch(11) train_loss: 0.2686 train_acc: 0.9015val_loss: 0.2539 val_acc: 0.9044\n",
      "epoch(12) train_loss: 0.2582 train_acc: 0.9054val_loss: 0.2453 val_acc: 0.9076\n",
      "epoch(13) train_loss: 0.2485 train_acc: 0.909val_loss: 0.2377 val_acc: 0.9101\n",
      "epoch(14) train_loss: 0.2395 train_acc: 0.9124val_loss: 0.231 val_acc: 0.9125\n",
      "epoch(15) train_loss: 0.2309 train_acc: 0.9156val_loss: 0.2246 val_acc: 0.915\n",
      "epoch(16) train_loss: 0.2229 train_acc: 0.9186val_loss: 0.2201 val_acc: 0.9166\n",
      "epoch(17) train_loss: 0.2153 train_acc: 0.9215val_loss: 0.2147 val_acc: 0.9186\n",
      "epoch(18) train_loss: 0.2081 train_acc: 0.9241val_loss: 0.2094 val_acc: 0.9206\n",
      "epoch(19) train_loss: 0.2015 train_acc: 0.9266val_loss: 0.2058 val_acc: 0.922\n",
      "epoch(20) train_loss: 0.1952 train_acc: 0.9289val_loss: 0.2016 val_acc: 0.9237\n",
      "epoch(21) train_loss: 0.1894 train_acc: 0.931val_loss: 0.1976 val_acc: 0.9253\n",
      "epoch(22) train_loss: 0.1837 train_acc: 0.933val_loss: 0.1931 val_acc: 0.927\n",
      "epoch(23) train_loss: 0.1786 train_acc: 0.9349val_loss: 0.1903 val_acc: 0.9283\n",
      "epoch(24) train_loss: 0.1737 train_acc: 0.9367val_loss: 0.1867 val_acc: 0.9298\n",
      "epoch(25) train_loss: 0.1689 train_acc: 0.9384val_loss: 0.183 val_acc: 0.9312\n",
      "epoch(26) train_loss: 0.1645 train_acc: 0.94val_loss: 0.1795 val_acc: 0.9326\n",
      "epoch(27) train_loss: 0.1602 train_acc: 0.9416val_loss: 0.1773 val_acc: 0.9336\n",
      "epoch(28) train_loss: 0.1562 train_acc: 0.943val_loss: 0.1741 val_acc: 0.9349\n",
      "epoch(29) train_loss: 0.1523 train_acc: 0.9444val_loss: 0.1716 val_acc: 0.9359\n",
      "epoch(30) train_loss: 0.1488 train_acc: 0.9457val_loss: 0.17 val_acc: 0.9368\n",
      "epoch(31) train_loss: 0.1455 train_acc: 0.9469val_loss: 0.1678 val_acc: 0.9377\n",
      "epoch(32) train_loss: 0.1421 train_acc: 0.9481val_loss: 0.1657 val_acc: 0.9385\n",
      "epoch(33) train_loss: 0.1389 train_acc: 0.9493val_loss: 0.1639 val_acc: 0.9393\n",
      "epoch(34) train_loss: 0.1359 train_acc: 0.9504val_loss: 0.1618 val_acc: 0.9402\n",
      "epoch(35) train_loss: 0.1329 train_acc: 0.9515val_loss: 0.1598 val_acc: 0.941\n",
      "epoch(36) train_loss: 0.1302 train_acc: 0.9525val_loss: 0.158 val_acc: 0.9418\n",
      "epoch(37) train_loss: 0.1274 train_acc: 0.9535val_loss: 0.1562 val_acc: 0.9426\n",
      "epoch(38) train_loss: 0.1248 train_acc: 0.9544val_loss: 0.1545 val_acc: 0.9433\n",
      "epoch(39) train_loss: 0.1224 train_acc: 0.9553val_loss: 0.1526 val_acc: 0.9441\n",
      "epoch(40) train_loss: 0.1202 train_acc: 0.9561val_loss: 0.1506 val_acc: 0.9449\n",
      "epoch(41) train_loss: 0.118 train_acc: 0.9569val_loss: 0.1486 val_acc: 0.9457\n",
      "epoch(42) train_loss: 0.1157 train_acc: 0.9578val_loss: 0.1464 val_acc: 0.9465\n",
      "epoch(43) train_loss: 0.1136 train_acc: 0.9585val_loss: 0.1451 val_acc: 0.9471\n",
      "epoch(44) train_loss: 0.1115 train_acc: 0.9593val_loss: 0.1432 val_acc: 0.9478\n",
      "epoch(45) train_loss: 0.1094 train_acc: 0.9601val_loss: 0.1416 val_acc: 0.9485\n",
      "epoch(46) train_loss: 0.1076 train_acc: 0.9608val_loss: 0.141 val_acc: 0.949\n",
      "epoch(47) train_loss: 0.1058 train_acc: 0.9614val_loss: 0.1399 val_acc: 0.9495\n",
      "epoch(48) train_loss: 0.104 train_acc: 0.9621val_loss: 0.1384 val_acc: 0.9502\n",
      "epoch(49) train_loss: 0.1023 train_acc: 0.9627val_loss: 0.1366 val_acc: 0.9508\n",
      "epoch(50) train_loss: 0.1005 train_acc: 0.9634val_loss: 0.1347 val_acc: 0.9516\n",
      "epoch(51) train_loss: 0.09898 train_acc: 0.9639val_loss: 0.1332 val_acc: 0.9522\n",
      "epoch(52) train_loss: 0.09746 train_acc: 0.9645val_loss: 0.1315 val_acc: 0.9528\n",
      "epoch(53) train_loss: 0.0959 train_acc: 0.9651val_loss: 0.1298 val_acc: 0.9534\n",
      "epoch(54) train_loss: 0.09446 train_acc: 0.9656val_loss: 0.1284 val_acc: 0.954\n",
      "epoch(55) train_loss: 0.09303 train_acc: 0.9661val_loss: 0.1268 val_acc: 0.9546\n",
      "epoch(56) train_loss: 0.09172 train_acc: 0.9666val_loss: 0.1251 val_acc: 0.9552\n",
      "epoch(57) train_loss: 0.09029 train_acc: 0.9671val_loss: 0.1234 val_acc: 0.9558\n",
      "epoch(58) train_loss: 0.08899 train_acc: 0.9676val_loss: 0.122 val_acc: 0.9564\n",
      "epoch(59) train_loss: 0.08777 train_acc: 0.9681val_loss: 0.1206 val_acc: 0.9569\n",
      "epoch(60) train_loss: 0.08663 train_acc: 0.9685val_loss: 0.1191 val_acc: 0.9575\n",
      "epoch(61) train_loss: 0.08554 train_acc: 0.9689val_loss: 0.1182 val_acc: 0.9578\n",
      "epoch(62) train_loss: 0.08434 train_acc: 0.9693val_loss: 0.1169 val_acc: 0.9583\n",
      "epoch(63) train_loss: 0.08341 train_acc: 0.9697val_loss: 0.1158 val_acc: 0.9588\n",
      "epoch(64) train_loss: 0.08235 train_acc: 0.9701val_loss: 0.1144 val_acc: 0.9593\n",
      "epoch(65) train_loss: 0.08125 train_acc: 0.9705val_loss: 0.1131 val_acc: 0.9598\n",
      "epoch(66) train_loss: 0.08022 train_acc: 0.9708val_loss: 0.1122 val_acc: 0.9601\n",
      "epoch(67) train_loss: 0.07927 train_acc: 0.9712val_loss: 0.111 val_acc: 0.9606\n",
      "epoch(68) train_loss: 0.0783 train_acc: 0.9716val_loss: 0.1099 val_acc: 0.961\n",
      "epoch(69) train_loss: 0.07734 train_acc: 0.9719val_loss: 0.1091 val_acc: 0.9614\n",
      "epoch(70) train_loss: 0.07647 train_acc: 0.9722val_loss: 0.1083 val_acc: 0.9617\n",
      "epoch(71) train_loss: 0.07555 train_acc: 0.9726val_loss: 0.107 val_acc: 0.9621\n",
      "epoch(72) train_loss: 0.07464 train_acc: 0.9729val_loss: 0.1061 val_acc: 0.9625\n",
      "epoch(73) train_loss: 0.07379 train_acc: 0.9732val_loss: 0.105 val_acc: 0.9629\n",
      "epoch(74) train_loss: 0.07296 train_acc: 0.9735val_loss: 0.1038 val_acc: 0.9633\n",
      "epoch(75) train_loss: 0.07211 train_acc: 0.9738val_loss: 0.1026 val_acc: 0.9638\n",
      "epoch(76) train_loss: 0.07135 train_acc: 0.9741val_loss: 0.1018 val_acc: 0.9641\n",
      "epoch(77) train_loss: 0.07061 train_acc: 0.9744val_loss: 0.1008 val_acc: 0.9645\n",
      "epoch(78) train_loss: 0.06982 train_acc: 0.9747val_loss: 0.09981 val_acc: 0.9648\n",
      "epoch(79) train_loss: 0.06908 train_acc: 0.975val_loss: 0.09892 val_acc: 0.9652\n",
      "epoch(80) train_loss: 0.06843 train_acc: 0.9752val_loss: 0.09815 val_acc: 0.9655\n",
      "epoch(81) train_loss: 0.06776 train_acc: 0.9755val_loss: 0.0973 val_acc: 0.9658\n",
      "epoch(82) train_loss: 0.0671 train_acc: 0.9757val_loss: 0.09648 val_acc: 0.9661\n",
      "epoch(83) train_loss: 0.06644 train_acc: 0.9759val_loss: 0.09583 val_acc: 0.9663\n",
      "epoch(84) train_loss: 0.06576 train_acc: 0.9762val_loss: 0.09486 val_acc: 0.9667\n",
      "epoch(85) train_loss: 0.06515 train_acc: 0.9764val_loss: 0.09405 val_acc: 0.967\n",
      "epoch(86) train_loss: 0.06448 train_acc: 0.9767val_loss: 0.09328 val_acc: 0.9673\n",
      "epoch(87) train_loss: 0.06388 train_acc: 0.9769val_loss: 0.09245 val_acc: 0.9676\n",
      "epoch(88) train_loss: 0.06333 train_acc: 0.9771val_loss: 0.09158 val_acc: 0.9679\n",
      "epoch(89) train_loss: 0.06273 train_acc: 0.9773val_loss: 0.09075 val_acc: 0.9682\n",
      "epoch(90) train_loss: 0.06217 train_acc: 0.9775val_loss: 0.09014 val_acc: 0.9685\n",
      "epoch(91) train_loss: 0.06163 train_acc: 0.9777val_loss: 0.08959 val_acc: 0.9687\n",
      "epoch(92) train_loss: 0.06105 train_acc: 0.9779val_loss: 0.08877 val_acc: 0.969\n",
      "epoch(93) train_loss: 0.06054 train_acc: 0.9781val_loss: 0.08811 val_acc: 0.9692\n",
      "epoch(94) train_loss: 0.06002 train_acc: 0.9783val_loss: 0.08736 val_acc: 0.9695\n",
      "epoch(95) train_loss: 0.05955 train_acc: 0.9785val_loss: 0.08669 val_acc: 0.9697\n",
      "epoch(96) train_loss: 0.05906 train_acc: 0.9787val_loss: 0.08607 val_acc: 0.97\n",
      "epoch(97) train_loss: 0.05855 train_acc: 0.9789val_loss: 0.08528 val_acc: 0.9702\n",
      "epoch(98) train_loss: 0.05807 train_acc: 0.9791val_loss: 0.08464 val_acc: 0.9705\n",
      "epoch(99) train_loss: 0.05759 train_acc: 0.9792val_loss: 0.0839 val_acc: 0.9707\n",
      "epoch(100) train_loss: 0.05708 train_acc: 0.9794val_loss: 0.08314 val_acc: 0.971\n",
      "Model: \"cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  640       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            multiple                  4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  1290      \n",
      "=================================================================\n",
      "Total params: 125,498\n",
      "Trainable params: 125,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wall time: 5h 38min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "8.モデルを生成して学習する\n",
    "'''\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# エポック数\n",
    "epochs = 100\n",
    "# ミニバッチのサイズ\n",
    "batch_size = 64\n",
    "# 訓練データのステップ数\n",
    "steps = x_train.shape[0] // batch_size\n",
    "# 検証データのステップ数\n",
    "steps_val = x_val.shape[0] // batch_size\n",
    "\n",
    "# 出力層10ニューロンのモデルを生成\n",
    "model = CNN(10)\n",
    "\n",
    "# 学習を行う\n",
    "for epoch in range(epochs):\n",
    "    # 訓練データと正解ラベルをシャッフル\n",
    "    x_, t_ = shuffle(x_train, t_train, random_state=1)\n",
    "    \n",
    "    # 1ステップにおけるミニバッチを使用した学習\n",
    "    for step in range(steps):\n",
    "        start = step * batch_size # ミニバッチの先頭インデックス\n",
    "        end = start + batch_size  # ミニバッチの末尾のインデックス\n",
    "        # ミニバッチでバイアス、重みを更新\n",
    "        train_step(x_[start:end], t_[start:end])\n",
    "        \n",
    "    # 検証データによるモデルの評価\n",
    "    for step_val in range(steps_val):\n",
    "        start = step_val * batch_size # ミニバッチの先頭インデックス\n",
    "        end = start + batch_size      # ミニバッチの末尾のインデックス\n",
    "        # 検証データのミニバッチで損失と精度を測定\n",
    "        val_step(x_[start:end], t_[start:end])\n",
    "\n",
    "    # 1エポックごとに結果を出力\n",
    "    print('epoch({}) train_loss: {:.4} train_acc: {:.4}'\n",
    "          'val_loss: {:.4} val_acc: {:.4}'.format(\n",
    "              epoch+1,\n",
    "              train_loss.result(), # 訓練データの損失を出力\n",
    "              train_acc.result(),  # 訓練データの精度を出力\n",
    "              val_loss.result(),   # 検証データの損失を出力\n",
    "              val_acc.result()     # 検証データの精度を出力\n",
    "              ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8189, test_acc: 0.9111\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "9. テストデータによるモデルの評価\n",
    "'''\n",
    "# 損失を記録するオブジェクトを生成\n",
    "test_loss = keras.metrics.Mean()\n",
    "# 精度を記録するオブジェクトを生成\n",
    "test_acc = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "def test_step(x, t):\n",
    "    '''テストデータをモデルに入力して損失と精度を測定\n",
    "    \n",
    "    Parameters: x(ndarray(float32)):テストデータ\n",
    "                t(ndarray(float32)):正解ラベル\n",
    "    '''\n",
    "    # テストデータの予測値を取得\n",
    "    preds = model(x)\n",
    "    # 出力値と正解ラベルの誤差\n",
    "    tmp_loss = loss(t, preds)\n",
    "    # 損失をMeanオブジェクトに記録\n",
    "    test_loss(tmp_loss)\n",
    "    # 精度をSpase_CategoricalAccuracyオブジェクトに記録\n",
    "    test_acc(t, preds)\n",
    "\n",
    "# テストデータで予測して損失と精度を取得\n",
    "test_step(x_test, t_test)\n",
    "\n",
    "print('test_loss: {:.4f}, test_acc: {:.4f}'.format(\n",
    "    test_loss.result(),\n",
    "    test_acc.result()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
