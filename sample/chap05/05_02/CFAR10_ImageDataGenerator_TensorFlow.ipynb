{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. データセットの読み込み\n",
    "'''\n",
    "# tensorflow.keras のインポート\n",
    "from tensorflow import keras\n",
    "\n",
    "# CIFAR-10データセットの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2.モデルの作成\n",
    "'''\n",
    "class CNN(keras.Model):\n",
    "    '''畳み込みニューラルネットワーク\n",
    "    \n",
    "    Attributes:\n",
    "      l1(Dense): 隠れ層\n",
    "      l2(Dense): 出力層\n",
    "    '''\n",
    "    def __init__(self, output_dim):\n",
    "        '''\n",
    "        Parameters:\n",
    "          output_dim(int): 出力層のユニット数(次元)\n",
    "          \n",
    "        Attributes:\n",
    "          c1(Conv2D): 畳み込み層1\n",
    "          p1(MaxPooling2D): プーリング層1\n",
    "          c2(Conv2D): 畳み込み層2\n",
    "          p2(MaxPooling2D): プーリング層2\n",
    "          c3(Conv2D): 畳み込み層3\n",
    "          p3(MaxPooling2D): プーリング層3\n",
    "          f1(Flatten): Flatten\n",
    "          d1(Dropout): ドロップアウト        \n",
    "          l1(Dense): 全結合層          \n",
    "          l2(Dense): 出力層\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # 正則化の係数\n",
    "        weight_decay = 1e-4\n",
    "\n",
    "        # (第1層)畳み込み層1 正則化を行う\n",
    "        self.c1 = keras.layers.Conv2D(\n",
    "            filters=32,                   # フィルター数32\n",
    "            kernel_size=(3, 3),           # 3×3のフィルター\n",
    "            padding='same',               # ゼロパディング\n",
    "            input_shape=x_train[0].shape, # 入力データの形状\n",
    "            kernel_regularizer=keras.regularizers.l2(\n",
    "                weight_decay), # 正則化\n",
    "            activation='relu'             # 活性化関数はReLU\n",
    "            )\n",
    "\n",
    "        # (第2層)プーリング層1：ウィンドウサイズは2×2\n",
    "        self.p1 = keras.layers.MaxPooling2D(\n",
    "            pool_size=(2, 2))             # 縮小対象の領域は2×2\n",
    "\n",
    "        # (第3層)畳み込み層2　正則化を行う\n",
    "        self.c2 = keras.layers.Conv2D(\n",
    "            filters=128,                  # フィルターの数は128\n",
    "            kernel_size=(3, 3),           # 3×3のフィルターを使用\n",
    "            padding='same',               # ゼロパディングを行う\n",
    "            kernel_regularizer=keras.regularizers.l2(\n",
    "                weight_decay),           # 正則化\n",
    "            activation='relu'             # 活性化関数はReLU\n",
    "            )\n",
    " \n",
    "        # (第4層)プーリング層2：ウィンドウサイズは2×2\n",
    "        self.p2 = keras.layers.MaxPooling2D(\n",
    "            pool_size=(2, 2))             # 縮小対象の領域は2×2       \n",
    "        \n",
    "        # (第5層)畳み込み層3　正則化を行う\n",
    "        self.c3 = keras.layers.Conv2D(\n",
    "            filters=256,                  # フィルターの数は256\n",
    "            kernel_size=(3, 3),           # 3×3のフィルターを使用\n",
    "            padding='same',               # ゼロパディングを行う\n",
    "            kernel_regularizer=keras.regularizers.l2(weight_decay), # 正則化\n",
    "            activation='relu'             # 活性化関数はReLU\n",
    "            )\n",
    "\n",
    "        # (第6層)プーリング層3：ウィンドウサイズは2×2\n",
    "        self.p3 = keras.layers.MaxPooling2D(\n",
    "            pool_size=(2, 2))             # 縮小対象の領域は2×2\n",
    "       \n",
    "        # Flaten\n",
    "        # ニューロン数＝4×4×256=4,096\n",
    "        # (4, 4, 256)を(,4096)にフラット化\n",
    "        self.f1 = keras.layers.Flatten()\n",
    "\n",
    "        # ドロップアウト：ドロップアウトは40％\n",
    "        self.d1 = keras.layers.Dropout(0.4)\n",
    "\n",
    "        # （第7層）全結合層\n",
    "        self.l1 =  keras.layers.Dense(\n",
    "            512,                          # 出力層のニューロン数は512\n",
    "            activation='relu')            # 活性化関数はReLU\n",
    "\n",
    "        # （第8層）出力層\n",
    "        self.l2 =  keras.layers.Dense(\n",
    "            10,                           # 出力層のニューロン数は10\n",
    "            activation='softmax')         # 活性化関数はソフトマックス\n",
    "        \n",
    "        # すべての層をリストにする\n",
    "        self.ls = [self.c1, self.p1, self.c2, self.p2, self.c3, self.p3,\n",
    "                   self.f1, self.d1, self.l1, self.l2]\n",
    "\n",
    "    def call(self, x):\n",
    "        '''CNNのインスタンスからコールバックされる関数\n",
    "        \n",
    "        Parameters: x(ndarray(float32)):訓練データ、または検証データ\n",
    "        Returns(float32): CNNの出力として要素数10の1階テンソル        \n",
    "        '''\n",
    "        for layer in self.ls:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3.損失関数の定義\n",
    "'''\n",
    "# 損失関数はスパースラベル対応クロスエントロピー誤差\n",
    "cce = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def loss(t, y):\n",
    "    '''損失関数\n",
    "    Parameters: t(ndarray(float32)):正解ラベル\n",
    "                y(ndarray(float32)):予測値\n",
    "                \n",
    "    Returns: クロスエントロピー誤差\n",
    "    '''\n",
    "    return cce(t, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4.オプティマイザーのオブジェクトの生成\n",
    "  損失と精度を測定するオブジェクトの生成\n",
    "'''\n",
    "# 勾配降下アルゴリズムを使用するオプティマイザーを生成\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# 損失を記録するオブジェクトを生成\n",
    "train_loss = keras.metrics.Mean()\n",
    "# 精度を記録するオブジェクトを生成\n",
    "train_acc = keras.metrics.SparseCategoricalAccuracy()\n",
    "# 検証時の損失を記録するオブジェクトを生成\n",
    "val_loss = keras.metrics.Mean()\n",
    "# 検証時の精度を記録するオブジェクトを生成\n",
    "val_acc = keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5.勾配降下アルゴリズムによるパラメーターの更新処理\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_step(x, t):\n",
    "    '''学習を1回行う\n",
    "    \n",
    "    Parameters: x(ndarray(float32)):訓練データ\n",
    "                t(ndarray(float32)):正解ラベル\n",
    "                \n",
    "    Returns:\n",
    "      ステップごとのクロスエントロピー誤差\n",
    "    '''\n",
    "    # 自動微分による勾配計算を記録するブロック\n",
    "    with tf.GradientTape() as tape:\n",
    "        # モデルに入力して順伝播の出力値を取得\n",
    "        outputs = model(x)\n",
    "        # 出力値と正解ラベルの誤差\n",
    "        tmp_loss = loss(t, outputs)\n",
    "        \n",
    "    # tapeに記録された操作を使用して誤差の勾配を計算        \n",
    "    grads = tape.gradient(\n",
    "        # 現在のステップの誤差\n",
    "        tmp_loss,\n",
    "        # バイアス、重みのリストを取得\n",
    "        model.trainable_variables)\n",
    "    # 勾配降下法の更新式を適用してバイアス、重みを更新\n",
    "    optimizer.apply_gradients(zip(grads,\n",
    "                                  model.trainable_variables))\n",
    "    \n",
    "    # 損失をMeanオブジェクトに記録\n",
    "    train_loss(tmp_loss)\n",
    "    # 精度をCategoricalAccuracyオブジェクトに記録\n",
    "    train_acc(t, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. 検証データによる評価を行う関数\n",
    "'''\n",
    "def val_step(x, t):\n",
    "    '''検証データをモデルに入力して損失と精度を測定\n",
    "    \n",
    "    Parameters: x(ndarray(float32)):検証データ\n",
    "                t(ndarray(float32)):正解ラベル\n",
    "    '''\n",
    "    # 検証データの予測値を取得\n",
    "    preds = model(x)\n",
    "    # 出力値と正解ラベルの誤差\n",
    "    tmp_loss = loss(t, preds)\n",
    "    # 損失をMeanオブジェクトに記録\n",
    "    val_loss(tmp_loss)\n",
    "    # 精度をSpase_CategoricalAccuracyオブジェクトに記録\n",
    "    val_acc(t, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch(1) train_loss: 1.93 train_acc: 0.3003 val_loss: 1.69 val_acc: 0.3708\n",
      "epoch(2) train_loss: 1.728 train_acc: 0.3754 val_loss: 1.561 val_acc: 0.4281\n",
      "epoch(3) train_loss: 1.607 train_acc: 0.4214 val_loss: 1.465 val_acc: 0.4669\n",
      "epoch(4) train_loss: 1.517 train_acc: 0.4551 val_loss: 1.401 val_acc: 0.4959\n",
      "epoch(5) train_loss: 1.443 train_acc: 0.4832 val_loss: 1.375 val_acc: 0.5055\n",
      "epoch(6) train_loss: 1.38 train_acc: 0.5065 val_loss: 1.319 val_acc: 0.5277\n",
      "epoch(7) train_loss: 1.325 train_acc: 0.5268 val_loss: 1.277 val_acc: 0.5438\n",
      "epoch(8) train_loss: 1.276 train_acc: 0.5453 val_loss: 1.232 val_acc: 0.5608\n",
      "epoch(9) train_loss: 1.232 train_acc: 0.5617 val_loss: 1.193 val_acc: 0.5753\n",
      "epoch(10) train_loss: 1.193 train_acc: 0.5759 val_loss: 1.167 val_acc: 0.5857\n",
      "epoch(11) train_loss: 1.158 train_acc: 0.5889 val_loss: 1.144 val_acc: 0.5949\n",
      "epoch(12) train_loss: 1.125 train_acc: 0.6006 val_loss: 1.117 val_acc: 0.6046\n",
      "epoch(13) train_loss: 1.096 train_acc: 0.6116 val_loss: 1.093 val_acc: 0.6139\n",
      "epoch(14) train_loss: 1.068 train_acc: 0.6216 val_loss: 1.076 val_acc: 0.6203\n",
      "epoch(15) train_loss: 1.042 train_acc: 0.6309 val_loss: 1.057 val_acc: 0.6277\n",
      "epoch(16) train_loss: 1.018 train_acc: 0.6396 val_loss: 1.04 val_acc: 0.634\n",
      "epoch(17) train_loss: 0.9954 train_acc: 0.6476 val_loss: 1.023 val_acc: 0.6402\n",
      "epoch(18) train_loss: 0.9745 train_acc: 0.6553 val_loss: 1.008 val_acc: 0.6456\n",
      "epoch(19) train_loss: 0.9542 train_acc: 0.6627 val_loss: 0.9977 val_acc: 0.65\n",
      "epoch(20) train_loss: 0.9354 train_acc: 0.6694 val_loss: 0.9851 val_acc: 0.6549\n",
      "epoch(21) train_loss: 0.9171 train_acc: 0.6759 val_loss: 0.9716 val_acc: 0.6598\n",
      "epoch(22) train_loss: 0.8999 train_acc: 0.682 val_loss: 0.9629 val_acc: 0.6633\n",
      "epoch(23) train_loss: 0.8837 train_acc: 0.6878 val_loss: 0.9516 val_acc: 0.6675\n",
      "epoch(24) train_loss: 0.8682 train_acc: 0.6933 val_loss: 0.9416 val_acc: 0.6714\n",
      "epoch(25) train_loss: 0.8532 train_acc: 0.6986 val_loss: 0.933 val_acc: 0.675\n",
      "epoch(26) train_loss: 0.8387 train_acc: 0.7039 val_loss: 0.9251 val_acc: 0.678\n",
      "epoch(27) train_loss: 0.8247 train_acc: 0.7087 val_loss: 0.9194 val_acc: 0.6807\n",
      "epoch(28) train_loss: 0.8116 train_acc: 0.7134 val_loss: 0.9123 val_acc: 0.6836\n",
      "epoch(29) train_loss: 0.7989 train_acc: 0.7179 val_loss: 0.9062 val_acc: 0.6862\n",
      "epoch(30) train_loss: 0.7866 train_acc: 0.7222 val_loss: 0.8999 val_acc: 0.6888\n",
      "epoch(31) train_loss: 0.7746 train_acc: 0.7264 val_loss: 0.8942 val_acc: 0.6911\n",
      "epoch(32) train_loss: 0.7632 train_acc: 0.7305 val_loss: 0.888 val_acc: 0.6935\n",
      "epoch(33) train_loss: 0.7521 train_acc: 0.7344 val_loss: 0.8832 val_acc: 0.6958\n",
      "epoch(34) train_loss: 0.7413 train_acc: 0.7383 val_loss: 0.8795 val_acc: 0.6977\n",
      "epoch(35) train_loss: 0.7311 train_acc: 0.7419 val_loss: 0.8755 val_acc: 0.6996\n",
      "epoch(36) train_loss: 0.7211 train_acc: 0.7454 val_loss: 0.8712 val_acc: 0.7018\n",
      "epoch(37) train_loss: 0.7112 train_acc: 0.749 val_loss: 0.8678 val_acc: 0.7037\n",
      "epoch(38) train_loss: 0.7017 train_acc: 0.7523 val_loss: 0.8639 val_acc: 0.7054\n",
      "epoch(39) train_loss: 0.6926 train_acc: 0.7556 val_loss: 0.8603 val_acc: 0.7071\n",
      "epoch(40) train_loss: 0.6837 train_acc: 0.7588 val_loss: 0.8564 val_acc: 0.7088\n",
      "epoch(41) train_loss: 0.675 train_acc: 0.7619 val_loss: 0.8531 val_acc: 0.7105\n",
      "epoch(42) train_loss: 0.6667 train_acc: 0.7648 val_loss: 0.8503 val_acc: 0.7121\n",
      "epoch(43) train_loss: 0.6586 train_acc: 0.7677 val_loss: 0.8474 val_acc: 0.7136\n",
      "epoch(44) train_loss: 0.6507 train_acc: 0.7705 val_loss: 0.8443 val_acc: 0.7154\n",
      "epoch(45) train_loss: 0.643 train_acc: 0.7732 val_loss: 0.8433 val_acc: 0.7166\n",
      "epoch(46) train_loss: 0.6354 train_acc: 0.7759 val_loss: 0.8422 val_acc: 0.7177\n",
      "epoch(47) train_loss: 0.6282 train_acc: 0.7785 val_loss: 0.8397 val_acc: 0.7192\n",
      "epoch(48) train_loss: 0.6212 train_acc: 0.7809 val_loss: 0.8379 val_acc: 0.7204\n",
      "epoch(49) train_loss: 0.6143 train_acc: 0.7834 val_loss: 0.8361 val_acc: 0.7216\n",
      "epoch(50) train_loss: 0.6076 train_acc: 0.7858 val_loss: 0.8349 val_acc: 0.7226\n",
      "epoch(51) train_loss: 0.6011 train_acc: 0.7881 val_loss: 0.834 val_acc: 0.7236\n",
      "epoch(52) train_loss: 0.5947 train_acc: 0.7903 val_loss: 0.8324 val_acc: 0.7246\n",
      "epoch(53) train_loss: 0.5884 train_acc: 0.7926 val_loss: 0.8313 val_acc: 0.7257\n",
      "epoch(54) train_loss: 0.5823 train_acc: 0.7947 val_loss: 0.83 val_acc: 0.7267\n",
      "epoch(55) train_loss: 0.5764 train_acc: 0.7968 val_loss: 0.8293 val_acc: 0.7275\n",
      "epoch(56) train_loss: 0.5706 train_acc: 0.7989 val_loss: 0.829 val_acc: 0.7282\n",
      "epoch(57) train_loss: 0.5649 train_acc: 0.8009 val_loss: 0.8286 val_acc: 0.7291\n",
      "epoch(58) train_loss: 0.5593 train_acc: 0.8029 val_loss: 0.8279 val_acc: 0.7301\n",
      "epoch(59) train_loss: 0.5537 train_acc: 0.8048 val_loss: 0.8271 val_acc: 0.7309\n",
      "epoch(60) train_loss: 0.5484 train_acc: 0.8067 val_loss: 0.8265 val_acc: 0.7318\n",
      "epoch(61) train_loss: 0.5432 train_acc: 0.8086 val_loss: 0.8262 val_acc: 0.7328\n",
      "epoch(62) train_loss: 0.538 train_acc: 0.8104 val_loss: 0.8257 val_acc: 0.7335\n",
      "epoch(63) train_loss: 0.5331 train_acc: 0.8121 val_loss: 0.8254 val_acc: 0.7342\n",
      "epoch(64) train_loss: 0.5282 train_acc: 0.8139 val_loss: 0.8252 val_acc: 0.735\n",
      "epoch(65) train_loss: 0.5235 train_acc: 0.8156 val_loss: 0.825 val_acc: 0.7358\n",
      "epoch(66) train_loss: 0.5189 train_acc: 0.8172 val_loss: 0.8242 val_acc: 0.7366\n",
      "epoch(67) train_loss: 0.5143 train_acc: 0.8188 val_loss: 0.824 val_acc: 0.7373\n",
      "epoch(68) train_loss: 0.5097 train_acc: 0.8205 val_loss: 0.8238 val_acc: 0.738\n",
      "epoch(69) train_loss: 0.5053 train_acc: 0.822 val_loss: 0.8239 val_acc: 0.7385\n",
      "epoch(70) train_loss: 0.501 train_acc: 0.8236 val_loss: 0.8245 val_acc: 0.7391\n",
      "epoch(71) train_loss: 0.4967 train_acc: 0.8251 val_loss: 0.8246 val_acc: 0.7397\n",
      "epoch(72) train_loss: 0.4926 train_acc: 0.8266 val_loss: 0.8249 val_acc: 0.7403\n",
      "epoch(73) train_loss: 0.4886 train_acc: 0.828 val_loss: 0.8251 val_acc: 0.7409\n",
      "epoch(74) train_loss: 0.4846 train_acc: 0.8294 val_loss: 0.825 val_acc: 0.7415\n",
      "epoch(75) train_loss: 0.4806 train_acc: 0.8308 val_loss: 0.825 val_acc: 0.7421\n",
      "epoch(76) train_loss: 0.4769 train_acc: 0.8322 val_loss: 0.8246 val_acc: 0.7427\n",
      "epoch(77) train_loss: 0.473 train_acc: 0.8335 val_loss: 0.8253 val_acc: 0.7432\n",
      "epoch(78) train_loss: 0.4694 train_acc: 0.8348 val_loss: 0.8257 val_acc: 0.7437\n",
      "epoch(79) train_loss: 0.4657 train_acc: 0.8361 val_loss: 0.8262 val_acc: 0.7443\n",
      "epoch(80) train_loss: 0.4622 train_acc: 0.8374 val_loss: 0.8262 val_acc: 0.7449\n",
      "epoch(81) train_loss: 0.4588 train_acc: 0.8386 val_loss: 0.8265 val_acc: 0.7454\n",
      "epoch(82) train_loss: 0.4553 train_acc: 0.8398 val_loss: 0.8277 val_acc: 0.7459\n",
      "epoch(83) train_loss: 0.452 train_acc: 0.841 val_loss: 0.8289 val_acc: 0.7462\n",
      "epoch(84) train_loss: 0.4487 train_acc: 0.8422 val_loss: 0.8292 val_acc: 0.7467\n",
      "epoch(85) train_loss: 0.4454 train_acc: 0.8434 val_loss: 0.8293 val_acc: 0.7472\n",
      "epoch(86) train_loss: 0.4423 train_acc: 0.8445 val_loss: 0.8297 val_acc: 0.7477\n",
      "epoch(87) train_loss: 0.4391 train_acc: 0.8456 val_loss: 0.8296 val_acc: 0.7482\n",
      "epoch(88) train_loss: 0.436 train_acc: 0.8467 val_loss: 0.8299 val_acc: 0.7487\n",
      "epoch(89) train_loss: 0.4329 train_acc: 0.8478 val_loss: 0.8306 val_acc: 0.7491\n",
      "epoch(90) train_loss: 0.4298 train_acc: 0.8489 val_loss: 0.8317 val_acc: 0.7494\n",
      "epoch(91) train_loss: 0.4269 train_acc: 0.85 val_loss: 0.8324 val_acc: 0.7498\n",
      "epoch(92) train_loss: 0.424 train_acc: 0.851 val_loss: 0.833 val_acc: 0.7502\n",
      "epoch(93) train_loss: 0.4211 train_acc: 0.852 val_loss: 0.8336 val_acc: 0.7507\n",
      "epoch(94) train_loss: 0.4183 train_acc: 0.853 val_loss: 0.8345 val_acc: 0.751\n",
      "epoch(95) train_loss: 0.4156 train_acc: 0.854 val_loss: 0.8347 val_acc: 0.7514\n",
      "epoch(96) train_loss: 0.4128 train_acc: 0.855 val_loss: 0.8353 val_acc: 0.7518\n",
      "epoch(97) train_loss: 0.4102 train_acc: 0.8559 val_loss: 0.8358 val_acc: 0.7521\n",
      "epoch(98) train_loss: 0.4076 train_acc: 0.8568 val_loss: 0.8371 val_acc: 0.7524\n",
      "epoch(99) train_loss: 0.405 train_acc: 0.8578 val_loss: 0.8378 val_acc: 0.7527\n",
      "epoch(100) train_loss: 0.4024 train_acc: 0.8587 val_loss: 0.8385 val_acc: 0.753\n",
      "Wall time: 6h 22min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "7.モデルを生成して学習する\n",
    "'''\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# エポック数\n",
    "epochs = 100\n",
    "# ミニバッチのサイズ\n",
    "batch_size = 64\n",
    "# ステップ数\n",
    "train_steps = len(x_train)*0.8 // batch_size\n",
    "# ステップ数\n",
    "val_steps = len(x_train)*0.2 // batch_size\n",
    "\n",
    "# 出力層10ニューロンのモデルを生成\n",
    "model = CNN(10)\n",
    "\n",
    "# ImageDataGeneratorを生成\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0/255.0,      # ピクセル値を255で割って正規化する\n",
    "    validation_split=0.2,   # 20パーセントのデータを検証用にする\n",
    "    rotation_range=15,      # 15度の範囲でランダムに回転させる\n",
    "    width_shift_range=0.1,  # 横サイズの0.1の割合でランダムに水平移動\n",
    "    height_shift_range=0.1, # 縦サイズの0.1の割合でランダムに垂直移動\n",
    "    horizontal_flip=True,  # 水平方向にランダムに反転、左右の入れ替え\n",
    "    zoom_range=0.2         # ランダムに拡大\n",
    ")\n",
    "\n",
    "# 訓練データ用のジェネレーターを生成\n",
    "training_generator = datagen.flow(x_train, t_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  subset='training')      # 訓練用のデータを生成\n",
    "# 検証データ用のジェネレーターを生成\n",
    "validation_generator = datagen.flow(x_train, t_train,\n",
    "                                    batch_size=batch_size,\n",
    "                                    subset='validation') # 検証用のデータを生成\n",
    "\n",
    "# 学習を行う\n",
    "for epoch in range(epochs):\n",
    "    # 訓練時のステップカウンター  \n",
    "    step_counter = 0  \n",
    "    # 1ステップ毎にミニバッチで学習する\n",
    "    for x_batch, t_batch in training_generator:\n",
    "        # ミニバッチでバイアス、重みを更新\n",
    "        train_step(x_batch, t_batch)\n",
    "        step_counter += 1\n",
    "        # すべてのステップが終了したらbreak\n",
    "        if step_counter >= train_steps:\n",
    "            break\n",
    "    \n",
    "    # 検証時のステップカウンター\n",
    "    v_step_counter = 0\n",
    "    # 検証データによるモデルの評価\n",
    "    for x_val_batch, t_val_batch  in validation_generator:\n",
    "        # 検証データのミニバッチで損失と精度を測定\n",
    "        val_step(x_val_batch, t_val_batch)\n",
    "        v_step_counter += 1\n",
    "        # すべてのステップが終了したらbreak\n",
    "        if v_step_counter >= val_steps:\n",
    "            break\n",
    "   \n",
    "    # 1エポックごとに結果を出力\n",
    "    print('epoch({}) train_loss: {:.4} train_acc: {:.4} '\n",
    "          'val_loss: {:.4} val_acc: {:.4}'.format(\n",
    "              epoch+1,\n",
    "              train_loss.result(), # 訓練データの損失を出力\n",
    "              train_acc.result(),  # 訓練データの精度を出力\n",
    "              val_loss.result(),   # 検証データの損失を出力\n",
    "              val_acc.result()     # 検証データの精度を出力\n",
    "              ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8192, test_acc: 0.8229\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "8. テストデータによるモデルの評価\n",
    "'''\n",
    "# 損失を記録するオブジェクトを生成\n",
    "test_loss = keras.metrics.Mean()\n",
    "# 精度を記録するオブジェクトを生成\n",
    "test_acc = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "def test_step(x, t):\n",
    "    '''テストデータをモデルに入力して損失と精度を測定\n",
    "    \n",
    "    Parameters: x(ndarray(float32)):テストデータ\n",
    "                t(ndarray(float32)):正解ラベル\n",
    "    '''\n",
    "    # テストデータの予測値を取得\n",
    "    preds = model(x)\n",
    "    # 出力値と正解ラベルの誤差\n",
    "    tmp_loss = loss(t, preds)\n",
    "    # 損失をMeanオブジェクトに記録\n",
    "    test_loss(tmp_loss)\n",
    "    # 精度をSpaseCategoricalAccuracyオブジェクトに記録\n",
    "    test_acc(t, preds)\n",
    "\n",
    "# テストデータを正規化\n",
    "x_test =  x_test.astype('float32') / 255\n",
    "# テストデータで予測して損失と精度を取得\n",
    "test_step(x_test, t_test)\n",
    "\n",
    "print('test_loss: {:.4f}, test_acc: {:.4f}'.format(\n",
    "    test_loss.result(),\n",
    "    test_acc.result()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
